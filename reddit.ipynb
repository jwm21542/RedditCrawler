{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/kpop/top/?t=hour\n",
      "PostLinks: [<selenium.webdriver.remote.webelement.WebElement (session=\"0af388f29df4503a7c40f40241ef956c\", element=\"f.C4040D450B7C92258AF33C0783245A6C.d.BD33E5545990523BE9BA86D2AFD12E02.e.43\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0af388f29df4503a7c40f40241ef956c\", element=\"f.C4040D450B7C92258AF33C0783245A6C.d.BD33E5545990523BE9BA86D2AFD12E02.e.96\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0af388f29df4503a7c40f40241ef956c\", element=\"f.C4040D450B7C92258AF33C0783245A6C.d.BD33E5545990523BE9BA86D2AFD12E02.e.98\")>]\n",
      "Element Href: /r/kpop/comments/1ca0au0/zico_block_b_11th_digital_single_spot_feat/\n",
      "Element Href: /r/kpop/comments/1ca098n/exo_love_fool_vcr_2024_fan_meeting_one/\n",
      "Element Href: /r/kpop/comments/1ca080i/tiot_reveals_their_official_fandomfanclub_name/\n",
      "Post: 1\n",
      "https://old.reddit.com/r/kpop/comments/1ca0au0/zico_block_b_11th_digital_single_spot_feat/\n",
      " I'm way too excited for this, Jennie looks incredible. \n",
      "Post: 2\n",
      "https://old.reddit.com/r/kpop/comments/1ca098n/exo_love_fool_vcr_2024_fan_meeting_one/\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import time as tm\n",
    "import random\n",
    "from user_agent import generate_user_agent, generate_navigator\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from selenium.webdriver.common.by import By\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--incognito')\n",
    "\n",
    "def lazy_scroll(driver):\n",
    "    current_height = driver.execute_script('return Math.max( document.body.scrollHeight, document.body.offsetHeight, document.documentElement.clientHeight, document.documentElement.scrollHeight, document.documentElement.offsetHeight );')\n",
    "    while True:\n",
    "        driver.execute_script('window.scrollTo(0,document.body.scrollHeight);')\n",
    "        tm.sleep(3)\n",
    "        new_height = driver.execute_script('return Math.max( document.body.scrollHeight, document.body.offsetHeight, document.documentElement.clientHeight, document.documentElement.scrollHeight, document.documentElement.offsetHeight );')\n",
    "        if new_height == current_height:\n",
    "            html = driver.page_source\n",
    "            break\n",
    "        current_height = new_height\n",
    "    return html\n",
    "\n",
    "def reddit_scrape(subreddit,driver):\n",
    "    url = subreddit\n",
    "    try : \n",
    "        driver.get(url)\n",
    "    except WebDriverException:\n",
    "        tm.sleep(10)\n",
    "        driver.quit()\n",
    "        driver = webdriver.Chrome(options=options, executable_path=ChromeDriverManager().install())\n",
    "        driver.get(url)\n",
    "\n",
    "    tm.sleep(5)\n",
    "    try:\n",
    "        driver.maximize_window()\n",
    "    except WebDriverException as e:\n",
    "        # Handle the specific exception when the window is already maximized\n",
    "        print(\"WebDriverException occurred while maximizing window:\", e)\n",
    "    \n",
    "    tm.sleep(5)\n",
    "    lazy_scroll(driver)\n",
    "    tm.sleep(5)\n",
    "    post_links = driver.find_elements(By.TAG_NAME, 'shreddit-post')\n",
    "\n",
    "    print('PostLinks: ' + str(post_links))\n",
    "\n",
    "    post_data = []\n",
    "    if(len(post_links) > 0):\n",
    "        for post in post_links:\n",
    "            href_attribute = post.get_attribute(\"permalink\")\n",
    "            print(f\"Element Href: {href_attribute}\")\n",
    "            post_data.append({'Permalink': href_attribute})\n",
    "\n",
    "        # Create a DataFrame from the list of dictionaries\n",
    "        df = pd.concat([pd.DataFrame(post_data)], ignore_index=True)\n",
    "        \n",
    "        result_df = pd.DataFrame(columns=['post_detail', 'platform', 'genre', 'post_like', 'post_created_time', 'post_source'])\n",
    "\n",
    "        postNum = 1\n",
    "\n",
    "        for post in df['Permalink']:\n",
    "            print(\"Post: \" + str(postNum))\n",
    "            postNum = postNum + 1\n",
    "            url = 'https://old.reddit.com' + post\n",
    "            print(url)\n",
    "            try : \n",
    "                driver.get(url)\n",
    "            except WebDriverException:\n",
    "                tm.sleep(10)\n",
    "                driver.quit()\n",
    "                #Depending on your OS, you may need to update creating the Driver instance!!\n",
    "                driver = webdriver.Chrome(options=options, executable_path=ChromeDriverManager().install())\n",
    "                driver.get(url)\n",
    "\n",
    "                \n",
    "            tm.sleep(3)\n",
    "            lazy_scroll(driver)\n",
    "            tm.sleep(2)\n",
    "            parent = driver.find_elements(By.CLASS_NAME, 'comment')\n",
    "\n",
    "            for element in parent:\n",
    "                comment = element.find_element(By.CLASS_NAME, 'tagline')\n",
    "                try:\n",
    "                    votescore = comment.find_element(By.CLASS_NAME, 'unvoted')\n",
    "                    score = votescore.get_attribute('title')\n",
    "                    print(score)\n",
    "                except:\n",
    "                    score = 0\n",
    "\n",
    "                time = comment.find_element(By.TAG_NAME, 'time')\n",
    "                orgTime = time.get_attribute('datetime')\n",
    "                original_time = datetime.strptime(orgTime, \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "                formatted_time_str = original_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                commentBox = element.find_element(By.CLASS_NAME, 'usertext-body')\n",
    "                commentText = commentBox.find_elements(By.TAG_NAME, 'p')\n",
    "                resultText = ''\n",
    "\n",
    "                for text in commentText:\n",
    "                    resultText = resultText + ' ' + text.text\n",
    "                print(resultText)\n",
    "                # Set values directly using loc accessor\n",
    "                result_df.loc[len(result_df)] = {\n",
    "                    'post_detail': resultText,\n",
    "                    'platform': 'Reddit',\n",
    "                    'post_like': score,\n",
    "                    'post_created_time': formatted_time_str,\n",
    "                    'post_source': url\n",
    "                }\n",
    "\n",
    "        result_df['post_detail'].replace('', np.nan, inplace=True)\n",
    "        result_df['post_detail'].replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "        result_df = result_df.dropna(subset=['post_detail'])\n",
    "        return result_df\n",
    "    \n",
    "subs = pd.read_excel('SubredditList.xlsx')\n",
    "subreddits = subs['url']\n",
    "final_df = pd.DataFrame(columns=['post_detail', 'platform', 'post_like', 'post_created_time', 'post_source'])\n",
    "for sub in subreddits:\n",
    "    driver = webdriver.Chrome(options=options, executable_path=ChromeDriverManager().install())\n",
    "    # Construct the URL for the subreddit. You may need to update this if Reddit changes the form of their URL. Also update week to hour, day, year, or all depending on what you need.\n",
    "    subreddit_url = f'{sub}/top/?t=week'\n",
    "    print(subreddit_url)\n",
    "    # Call the 'reddit_scrape' function for the current subreddit\n",
    "    subreddit_df = reddit_scrape(subreddit_url,driver)\n",
    "    # Append the resulting DataFrame to the final DataFrame\n",
    "    final_df = pd.concat([final_df, subreddit_df], ignore_index=True)\n",
    "    driver.quit()\n",
    "final_df.to_csv('RedditCrawlerResults.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
